import airsim
import numpy as np
import math
import time
import pandas as pd
import cv2
import torch
import threading
from airsim import Vector3r, to_quaternion, Pose
import atexit
from ultralytics import YOLO

from deep_sort_realtime.deepsort_tracker import DeepSort
from filterpy.kalman import KalmanFilter
from collections import defaultdict

# === ëª¨ë¸ ì„¸íŒ… ===
model = YOLO(
    "E:/AirSim_vscode/weights/yolov11m_myeong_best_airsim100.pt"
)  # yolov11m_myeong_best


# ìˆ˜í‰ ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜
# ì¹´ë©”ë¼ í•˜í–¥ê°ê³¼ ì´ë¯¸ì§€ì† ê°ì²´ ìœ„ì¹˜ë¡œ ì‚¼ê°í•¨ìˆ˜(tan)ì„ ì´ìš©í•´ì„œ ìˆ˜í‰ê±°ë¦¬ ì¶”ì •
# -------------------------
# def estimate_horizontal_distance_from_object(
#     drone_altitude_m,
#     camera_pitch_deg,
#     object_center_y_px,
#     image_height_px,
#     vertical_fov_deg,
# ):
#     deg_per_pixel = vertical_fov_deg / image_height_px
#     offset_pixel = object_center_y_px - (image_height_px / 2)
#     offset_angle_deg = offset_pixel * deg_per_pixel
#     total_angle_deg = camera_pitch_deg + offset_angle_deg
#     total_angle_rad = math.radians(total_angle_deg)
#     if total_angle_rad <= 0:
#         return None  # or float('inf')
#     horizontal_distance_m = drone_altitude_m / math.tan(total_angle_rad)
#     return round(horizontal_distance_m, 2)


# ì•„ë˜ëŠ” ìˆ˜í‰ê¹Œì§€ ê³ ë ¤í•œ í•¨ìˆ˜
def estimate_horizontal_distance_from_object(
    drone_altitude_m,
    camera_pitch_deg,
    camera_yaw_deg,
    object_center_x_px,
    object_center_y_px,
    image_width_px,
    image_height_px,
    horizontal_fov_deg,
    vertical_fov_deg,
):
    # FOV ê¸°ì¤€ìœ¼ë¡œ ê° í”½ì…€ë‹¹ ê°ë„ ê³„ì‚°
    deg_per_px_y = vertical_fov_deg / image_height_px
    deg_per_px_x = horizontal_fov_deg / image_width_px

    # ì´ë¯¸ì§€ ì¤‘ì•™ìœ¼ë¡œë¶€í„°ì˜ ì˜¤í”„ì…‹
    offset_y_px = object_center_y_px - (image_height_px / 2)
    offset_x_px = object_center_x_px - (image_width_px / 2)

    # í•´ë‹¹ í”½ì…€ì˜ pitch, yaw ì˜¤í”„ì…‹
    offset_pitch_deg = offset_y_px * deg_per_px_y
    offset_yaw_deg = offset_x_px * deg_per_px_x

    # ì´ pitch, yaw ê°ë„
    total_pitch_deg = camera_pitch_deg + offset_pitch_deg
    total_yaw_deg = camera_yaw_deg + offset_yaw_deg

    # pitch â†’ ìˆ˜í‰ ê±°ë¦¬
    total_pitch_rad = math.radians(total_pitch_deg)
    if total_pitch_rad <= 0:
        # raise ValueError("ì´ pitchê°€ 0ë„ ì´í•˜ì…ë‹ˆë‹¤. ì¹´ë©”ë¼ ê°ë„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None

    base_distance = drone_altitude_m / math.tan(total_pitch_rad)

    # yaw ê°ë„ ë³´ì • (ì¸¡ë©´ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ê±°ë¦¬ ì¦ê°€)
    yaw_correction = math.cos(math.radians(total_yaw_deg))
    if yaw_correction <= 0.01:  # ë„ˆë¬´ ì¸¡ë©´ì¸ ê²½ìš° ë°©ì§€
        yaw_correction = 0.01

    corrected_distance = base_distance / yaw_correction
    return round(corrected_distance, 2)


# ì¶”ì í•  íŠ¹ì • ID (ì´ˆê¸°ê°’: None)
target_track_id = None
# ë§ˆìš°ìŠ¤ í´ë¦­ìœ¼ë¡œ íŠ¹ì • ì‚¬ëŒ ì„ íƒ
clicked_point = None  # ì‚¬ìš©ìê°€ í´ë¦­í•œ ì¢Œí‘œ ì €ì¥


### "í•œ ë²ˆ ë” í´ë¦­í•˜ë©´ ì¶”ì  í•´ì œ" ê¸°ëŠ¥ì„ ì¶”ê°€
def select_target(event, x, y, flags, param):
    global clicked_point, target_track_id
    if event == cv2.EVENT_LBUTTONDOWN:
        if target_track_id is not None:
            # ğŸ‘‰ ì´ë¯¸ ì¶”ì  ì¤‘ì´ë©´ í•´ì œ
            print(f"âŒ ì¶”ì  í•´ì œ (ID {target_track_id})")
            target_track_id = None
            clicked_point = None
        else:
            # ğŸ‘‰ ì¶”ì  ëŒ€ìƒì´ ì—†ìœ¼ë©´ ì„ íƒ ì‹œì‘
            clicked_point = (x, y)


stop_event = threading.Event()


def yolo_worker_ultralytics():
    global clicked_point
    print("[YOLO] Ultralytics YOLO ê°ì²´ íƒì§€ ìŠ¤ë ˆë“œ ì‹œì‘")
    yolo_client = airsim.MultirotorClient()
    yolo_client.confirmConnection()

    # âœ… ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •
    output_path = "output_video/airsimë“œë¡ _ì£¼ë‘”ì§€ê²½ê³„.mp4"
    fps = 1
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(output_path, fourcc, fps, (1280, 720))

    while not stop_event.is_set():
        try:
            ## ê¸°ì¡´ ì½”ë“œ ###
            response = yolo_client.simGetImage(
                "front_center", airsim.ImageType.Scene, vehicle_name="Drone1"
            )

            if response:
                # Decode RGB
                img_array = np.frombuffer(response, dtype=np.uint8)
                frame = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                height, width, _ = frame.shape

                ### ê±°ë¦¬ í•¨ìˆ˜ íŒŒë¼ë¯¸í„°
                state = yolo_client.getMultirotorState(vehicle_name="Drone1")
                DRONE_ALTITUDE = -state.kinematics_estimated.position.z_val
                # ê±°ë¦¬ ê³„ì‚°ì— í•„ìš”í•œ ì¹´ë©”ë¼ ì •ë³´ ìë™ ì¶”ì¶œ
                cam_info = yolo_client.simGetCameraInfo("front_center")
                pitch_rad, _, yaw_rad = airsim.to_eularian_angles(
                    cam_info.pose.orientation
                )

                # ë“œë¡  ê¸°ì¤€, ì¹´ë©”ë¼ê°€ ì•„ë˜ë¥¼ ë³´ë©´ ìŒìˆ˜, ìœ„ë¥¼ ë³´ë©´ ì–‘ìˆ˜ â†’ ì‚¼ê°ë²• ìˆ˜ì‹ê³¼ ë°©í–¥ì´ ë°˜ëŒ€
                # math.degreesê°’ ë°˜ì „ í•„ìš”
                CAMERA_YAW_DEG = math.degrees(yaw_rad)
                CAMERA_PITCH_DEG = -math.degrees(pitch_rad)  # pitch angle in degrees
                VERTICAL_FOV_DEG = cam_info.fov  # vertical FOV in degrees
                IMAGE_WIDTH_PX = width
                IMAGE_HEIGHT_PX = height
                HORIZONTAL_FOV_DEG = 2 * math.degrees(
                    math.atan(
                        math.tan(math.radians(VERTICAL_FOV_DEG / 2))
                        * (IMAGE_WIDTH_PX / IMAGE_HEIGHT_PX)
                    )
                )

                FOCAL_LENGTH_PX = 800  # ì˜ˆì‹œê°’, ì‹¤ì œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš”

                # DeepSORT íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
                tracker = DeepSort(max_age=30)

                # Kalman Filter ì´ˆê¸°í™”
                kf = KalmanFilter(dim_x=4, dim_z=2)  # 4D ìƒíƒœ(ìœ„ì¹˜+ì†ë„), 2D ê´€ì¸¡(ìœ„ì¹˜)
                kf.F = np.array(
                    [
                        [1, 0, 1, 0],  # x = x + vx    / ìƒíƒœ ì „ì´ í–‰ë ¬ ì‚¬ìš©
                        [0, 1, 0, 1],  # y = y + vy
                        [0, 0, 1, 0],  # vx ìœ ì§€
                        [0, 0, 0, 1],
                    ]
                )  # vy ìœ ì§€
                kf.H = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])  # xë§Œ ê´€ì¸¡  # yë§Œ ê´€ì¸¡
                kf.P *= 1000  # ì´ˆê¸° ë¶ˆí™•ì‹¤ì„±
                kf.R *= 10  # ì¸¡ì • ë…¸ì´ì¦ˆ

                cv2.namedWindow("YOLO + DeepSORT Tracking")
                cv2.setMouseCallback("YOLO + DeepSORT Tracking", select_target)

                if frame is not None:
                    # â© Ultralytics ëª¨ë¸ ì¶”ë¡ 
                    results = model(frame)[0]  # ì²« ë²ˆì§¸ ê²°ê³¼
                    print("[YOLO] ë°•ìŠ¤ ê°œìˆ˜:", len(results.boxes))
                    detections = []

                    # âœ… ì¶”ì í•  í´ë˜ìŠ¤ ì§€ì •
                    track_classes = [0, 2, 7]  # person, car, truck

                    # âœ… ê°ì²´ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                    detected_objects_info = []
                    class_counter = defaultdict(int)  # âœ… ê°ì²´ë³„ ìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬

                    ### í…ŒìŠ¤íŠ¸ìš© ###
                    for result in results.boxes.data:
                        x1, y1, x2, y2, score, class_id = result.tolist()
                        center_x = int((x1 + x2) / 2)
                        center_y = int((y1 + y2) / 2)
                        class_id = int(class_id)
                        label = (
                            model.names[class_id]
                            if class_id in model.names
                            else "Unknown"
                        )
                        distance_to_object = estimate_horizontal_distance_from_object(
                            DRONE_ALTITUDE,
                            CAMERA_PITCH_DEG,
                            CAMERA_YAW_DEG,
                            center_x,
                            center_y,
                            IMAGE_WIDTH_PX,
                            IMAGE_HEIGHT_PX,
                            HORIZONTAL_FOV_DEG,
                            VERTICAL_FOV_DEG,
                        )
                        if distance_to_object is None:
                            continue  # ê±°ë¦¬ ì¶”ì • ë¶ˆê°€í•œ ê°ì²´ëŠ” ìŠ¤í‚µí•˜ê±°ë‚˜ í‘œì‹œ ì œì™¸

                        # ğŸ‘‰ íƒì§€ëœ ê°ì²´ ì €ì¥
                        detected_objects_info.append(
                            {
                                "label": label,
                                "distance": distance_to_object,
                                "score": round(score, 2),
                            }
                        )
                        # âœ… ê°œìˆ˜ ì„¸ê¸°
                        class_counter[label] += 1

                        if class_id in track_classes and score > 0.5:
                            detections.append(
                                ([x1, y1, x2 - x1, y2 - y1], score, class_id)
                            )
                    # âœ… íƒì§€ëœ ê°ì²´ ìˆ˜ ë° ì •ë³´ ì¶œë ¥
                    print(f"[YOLO] âœ… íƒì§€ëœ ê°ì²´ ìˆ˜: {len(detected_objects_info)}")
                    for label, count in class_counter.items():
                        print(f" - {label}: {count}ê°œ")

                    tracked_objects = tracker.update_tracks(detections, frame=frame)
                    print(f"tracked_objects : {tracked_objects}")

                    global target_track_id
                    if clicked_point is not None and target_track_id is None:
                        print(f"target_track_id : {target_track_id}")
                        print(f"clicked_point : {clicked_point}")
                        min_dist = float("inf")
                        for track in tracked_objects:
                            ltrb = track.to_ltrb()
                            x1, y1, x2, y2 = map(int, ltrb)
                            person_center = ((x1 + x2) // 2, (y1 + y2) // 2)

                            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
                            distance = np.linalg.norm(
                                np.array(person_center) - np.array(clicked_point)
                            )
                            if distance < min_dist:
                                min_dist = distance
                                target_track_id = track.track_id

                        print(f"ğŸ” ì„ íƒëœ ì¶”ì  ëŒ€ìƒ ID: {target_track_id}")
                        clicked_point = None  # í•œ ë²ˆ ì„ íƒí•˜ë©´ ë” ì´ìƒ í´ë¦­ ì•ˆ ë°›ìŒ

                    target_found = (
                        False  # ì¶”ì  ëŒ€ìƒì´ í˜„ì¬ í”„ë ˆì„ì—ì„œ ê°ì§€ë˜ì—ˆëŠ”ì§€ ì²´í¬
                    )

                    for track in tracked_objects:
                        track_id = track.track_id
                        print(f"track_id : {track_id}")
                        ltrb = track.to_ltrb()  # ì¢Œí‘œ ë³€í™˜
                        x1, y1, x2, y2 = map(int, ltrb)
                        person_center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])

                        if target_track_id is None:
                            # ì•„ì§ ê°ì²´ ì„ íƒ ì „ â†’ ëª¨ë“  ì‚¬ëŒì—ê²Œ ë°•ìŠ¤ í‘œì‹œ
                            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)
                            cv2.putText(
                                frame,
                                f"{label}",
                                (x1, y1 - 5),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                0.5,
                                (0, 255, 0),
                                1,
                            )
                        elif track_id == target_track_id:
                            # ì„ íƒëœ ëŒ€ìƒë§Œ ì¶”ì 
                            target_found = True
                            kf.update(person_center)

                            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                            cv2.putText(
                                frame,
                                f"Target ID {track_id} / Distance:{distance_to_object}m",
                                (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                0.7,
                                (0, 255, 0),
                                2,
                            )

                    # # ë§Œì•½ ì¶”ì  ëŒ€ìƒì´ ì‚¬ë¼ì¡Œë‹¤ë©´ â†’ Kalman Filterë¥¼ ì´ìš©í•´ ì˜ˆìƒ ìœ„ì¹˜ ê·¸ë¦¬ê¸°
                    # if not target_found and target_track_id is not None:
                    #     kf.predict()  # ê°ì²´ì˜ ì´ë™ ì˜ˆì¸¡
                    #     predicted_x, predicted_y = kf.x[:2]  # ì˜ˆì¸¡ëœ ìœ„ì¹˜
                    #     pred_point = int(predicted_x), int(predicted_y)

                    #     min_dist = float("inf")
                    #     for track in tracked_objects:
                    #         ltrb = track.to_ltrb()
                    #         x1, y1, x2, y2 = map(int, ltrb)
                    #         person_center = ((x1 + x2) // 2, (y1 + y2) // 2)

                    #         # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
                    #         distance = np.linalg.norm(
                    #             np.array(person_center) - np.array(pred_point)
                    #         )
                    #         if distance < min_dist:
                    #             min_dist = distance
                    #             target_track_id = (
                    #                 track.track_id
                    #             )  # ê°€ì¥ ê°€ê¹Œìš´ ì‚¬ëŒì˜ IDë¥¼ ì €ì¥

                    #     cv2.circle(
                    #         frame,
                    #         (int(predicted_x), int(predicted_y)),
                    #         10,
                    #         (0, 0, 255),
                    #         -1,
                    #     )  # ë¹¨ê°„ ì ìœ¼ë¡œ ì˜ˆì¸¡ ìœ„ì¹˜ í‘œì‹œ
                    #     cv2.putText(
                    #         frame,
                    #         "Predicted Position",
                    #         (int(predicted_x), int(predicted_y) - 10),
                    #         cv2.FONT_HERSHEY_SIMPLEX,
                    #         0.7,
                    #         (0, 0, 255),
                    #         2,
                    #     )

                    # í™”ë©´ ì¶œë ¥
                    cv2.imshow("YOLO + DeepSORT Tracking", frame)

                    out.write(frame)

                    # ESC í‚¤ ì¢…ë£Œ
                    if cv2.waitKey(1) & 0xFF == 27:
                        break

        except Exception as e:
            print(f"[YOLO] ì˜¤ë¥˜: {e}")
        time.sleep(0.2)

    out.release()  # â— ì˜ìƒ íŒŒì¼ ë‹«ê¸°
    cv2.destroyAllWindows()


def find_nearest_waypoint_index(current_xyz, waypoints):
    """í˜„ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ê²½ìœ ì§€ ì¸ë±ìŠ¤ ì°¾ê¸°"""
    dists = np.linalg.norm(waypoints[:, :2] - current_xyz[:2], axis=1)
    return np.argmin(dists)


def run_path(airsim_path, velocity):
    """ê²½ë¡œ ì „ì²´ë¥¼ moveOnPathAsyncë¡œ ì´ë™ (ìŠ¤ë ˆë“œìš©)"""
    local_client = airsim.MultirotorClient()
    local_client.confirmConnection()
    print(f"[moveOnPath] {len(airsim_path)}ê°œ ê²½ìœ ì§€ë¡œ ë¹„í–‰ ì‹œì‘")
    local_client.moveOnPathAsync(
        airsim_path,
        velocity=velocity,
        drivetrain=airsim.DrivetrainType.ForwardOnly,
        yaw_mode=airsim.YawMode(False, 0),
        vehicle_name="Drone1",
    ).join()
    print("[moveOnPath] moveOnPathAsync ì¢…ë£Œ")


# --- AirSim ë“œë¡  ì—°ê²° ---
client = airsim.MultirotorClient()
client.confirmConnection()
client.enableApiControl(True, vehicle_name="Drone1")
client.armDisarm(True, vehicle_name="Drone1")
client.takeoffAsync(vehicle_name="Drone1").join()
print("[ì‹œì‘] ë“œë¡  ì—°ê²° ë° ì´ë¥™ ì™„ë£Œ")

# --- fence_path.csvì—ì„œ x, y, z ì½ê¸° ë° ë³€í™˜ ---
df = pd.read_csv("fence_path.csv", header=0, names=["name", "x", "y", "z"])
waypoints = df[["x", "y", "z"]].values
waypoints[:, 2] = -waypoints[:, 2]  # zê°’ ë¶€í˜¸ ë°˜ì „ (AirSimì€ zê°€ ìŒìˆ˜ì¼ìˆ˜ë¡ ìœ„)
waypoints = waypoints / 100  # (cm â†’ m ë³€í™˜)
print(f"[ë¡œë“œ] ê²½ìœ ì§€ ìƒ˜í”Œ:\n{waypoints[:10]}")
print(f"[ë¡œë“œ] NaN í¬í•¨ ì—¬ë¶€: {np.isnan(waypoints).any()}")

obstacle_dist = 0.8  # ì¥ì• ë¬¼ íŒì • ê±°ë¦¬(m)
fov_deg = 90  # ì¥ì• ë¬¼ ê°ì§€ ê°ë„(ì „ë°© 120ë„)
safety_margin = 20  # íœìŠ¤ì—ì„œ ì¶”ê°€ë¡œ ëª‡ m ìœ„ë¥¼ ë‚ ì§€
velocity = 2

# --- ì¶œë°œì  í…”ë ˆí¬íŠ¸ ë° ì´ë¥™ ---
start = waypoints[0]
start_z = start[2] - safety_margin
print(f"ğŸ”¸ ì¶œë°œì ìœ¼ë¡œ í…”ë ˆí¬íŠ¸: x={start[0]:.2f}, y={start[1]:.2f}, z={start_z:.2f}")
client.simSetVehiclePose(
    Pose(
        Vector3r(float(start[0]), float(start[1]), float(start_z)),
        to_quaternion(0, 0, 0),
    ),
    ignore_collision=True,
    vehicle_name="Drone1",
)
time.sleep(1)
client.hoverAsync(vehicle_name="Drone1").join()
print("[ì‹œì‘] ë“œë¡  ìœ„ì¹˜ ì´ë™ ë° í˜¸ë²„ ì™„ë£Œ")

# --- YOLO ìŠ¤ë ˆë“œ ì‹œì‘ ---
yolo_thread = threading.Thread(target=yolo_worker_ultralytics, daemon=True)
yolo_thread.start()
print("[YOLO] íƒì§€ ìŠ¤ë ˆë“œ ì‹œì‘")


def patrol_loop():
    """ì „ì²´ ê²½ë¡œë¥¼ moveOnPathAsyncë¡œ ìˆœì°°, ì¥ì• ë¬¼ íƒì§€ ë° íšŒí”¼"""
    global waypoints, target_track_id
    while True:
        # ğŸ‘‰ ê°ì²´ë¥¼ í´ë¦­í•˜ì—¬ target_track_idê°€ ì¡´ì¬í•˜ë©´ ì¼ì‹œ ì •ì§€
        if target_track_id is not None:
            print(f"ğŸ›‘ ê°ì²´(ID: {target_track_id}) ì„ íƒë¨ â€” ë“œë¡  Hover ë° ì´ë™ ì¤‘ë‹¨")
            client.cancelLastTask(vehicle_name="Drone1")  # ì´ë™ ëª…ë ¹ ì·¨ì†Œ
            client.hoverAsync(vehicle_name="Drone1").join()
            time.sleep(0.5)
            continue  # ê³„ì† ëŒ€ê¸° (í´ë¦­ í•´ì œ ì „ê¹Œì§€)

        # í˜„ì¬ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ waypointë¶€í„° ê²½ë¡œ ì‹œì‘
        state = client.getMultirotorState(vehicle_name="Drone1")
        drone_xyz = np.array(
            [
                state.kinematics_estimated.position.x_val,
                state.kinematics_estimated.position.y_val,
                state.kinematics_estimated.position.z_val,
            ]
        )
        start_idx = find_nearest_waypoint_index(drone_xyz, waypoints)

        # ê²½ë¡œë¥¼ ì‹œì‘ ì¸ë±ìŠ¤ë¶€í„° ì¬êµ¬ì„±
        reordered_waypoints = np.concatenate(
            (waypoints[start_idx:], waypoints[:start_idx]), axis=0
        )
        # ğŸ‘‰ í´ë¦­ í•´ì œëœ ê²½ìš° ê²½ë¡œ ìˆœì°° ì‹œì‘
        airsim_path = [
            Vector3r(float(x), float(y), float(z) - safety_margin)
            for x, y, z in reordered_waypoints
        ]

        print(
            f"\n[moveOnPathAsync] {len(airsim_path)}ê°œ ì§€ì ìœ¼ë¡œ ìˆœì°° ì‹œì‘! (from idx {start_idx})"
        )
        path_thread = threading.Thread(target=run_path, args=(airsim_path, velocity))
        path_thread.start()

        while path_thread.is_alive():
            # í´ë¦­ ì‹œ ì‹¤ì‹œê°„ ì¤‘ë‹¨
            if target_track_id is not None:
                print("ğŸ” ì¤‘ê°„ ì •ì§€ ìš”ì²­ë¨ â€” moveOnPathAsync ì¤‘ë‹¨ ë° Hover")
                client.cancelLastTask(vehicle_name="Drone1")
                client.hoverAsync(vehicle_name="Drone1").join()
                break

            time.sleep(0.1)

        print("\nâœ… ì „ì²´ ê²½ë¡œ ìˆœì°° ì™„ë£Œ ë˜ëŠ” ì¤‘ë‹¨ë¨ â€” ë£¨í”„ ì¬ì‹œì‘")

        #     # ì•„ë˜ ì½”ë“œëŠ” ë“œë¡ ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì•„ì™€ì„œ,
        #     # ë‹¤ìŒ waypoint ë°©í–¥ì„ ê³„ì‚°í•˜ê³  heading_rad (ì§„í–‰ ë°©í–¥ ê°ë„)ë¥¼ êµ¬í•˜ëŠ” ë¶€ë¶„
        #     # í•˜ì§€ë§Œ moveOnPathAsync()ê°€ ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ë”°ë¼ê°€ê¸° ë•Œë¬¸ì—,
        #     # ë³„ë„ë¡œ heading ê°’ì„ ì§ì ‘ êµ¬í•´ì„œ ì¡°ì‘í•  í•„ìš”ëŠ” ì—†ìŒ
        #     # heading_rad ê°’ì€ í˜„ì¬ ì½”ë“œì—ì„œ ì•„ë¬´ ë°ì„œë„ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
        #     state = client.getMultirotorState(vehicle_name="Drone1")
        #     drone_xyz = np.array(
        #         [
        #             state.kinematics_estimated.position.x_val,
        #             state.kinematics_estimated.position.y_val,
        #             state.kinematics_estimated.position.z_val,
        #         ]
        #     )
        #     # ë§Œì•½ í–¥í›„ "ë‹¤ìŒ ë°©í–¥ ê°ë„"ë¥¼ HUDì— í‘œì‹œí•˜ê±°ë‚˜ ë“œë¡  ë°©í–¥ ì œì–´ì— ì‚¬ìš©í•  ìƒê°ì´ ìˆë‹¤ë©´,
        #     # ìœ„ ë¶€ë¶„ë§Œ ë‚¨ê²¨ë„ ë¨!!!

        #     prev_idx = find_nearest_waypoint_index(drone_xyz, waypoints)
        #     if prev_idx + 1 < len(waypoints):
        #         next_xyz = waypoints[prev_idx + 1]
        #     else:
        #         next_xyz = waypoints[0]
        #     diff = next_xyz[:2] - drone_xyz[:2]
        #     heading_rad = np.arctan2(diff[1], diff[0])

        #     time.sleep(0.1)
        # print("\nâœ… ì „ì²´ ê²½ë¡œ ìˆœì°° ì™„ë£Œ, ë‹¤ì‹œ ì‹œì‘")


def cleanup():
    client = airsim.MultirotorClient()
    # client.confirmConnection()
    client.armDisarm(False, vehicle_name="Drone1")
    client.enableApiControl(False, vehicle_name="Drone1")
    print(f"[Drone1] ì¢…ë£Œ ì™„ë£Œ")


atexit.register(cleanup)

# --- íŒ¨íŠ¸ë¡¤ ì‹œì‘ ---
try:
    print("[ìˆœì°°] ë“œë¡  1ëŒ€ ìˆœì°° ì‹œìŠ¤í…œ ì‹œì‘")
    patrol_loop()
except KeyboardInterrupt:
    print("ğŸ›‘ ì‚¬ìš©ì ì¢…ë£Œ ìš”ì²­")
    cleanup()
